// Test script for OpenAIStreamingWebClient
import { OpenAIStreamingWebClient, createStreamingClient } from './streaming-client';
import { EnhancedStreamingClient, StreamingModelManager } from './streaming-example';

async function testBasicStreaming() {
  console.log('ğŸ§ª Testing Basic Streaming Client\n');

  const client = createStreamingClient({
    apiKey: '651192c5-37ff-440a-b930-7444c69f4422',
    tenantId: 'core/falcontest1-core4sdb6/00DSG000002tHLd2AM',
    baseUrl: 'https://test.api.salesforce.com',
    featureId: 'EinsteinForDevelopers'
  });

  try {
    console.log('ğŸ“¤ Sending request...');
    console.log('ğŸ“ Prompt: "Write hello world in Javascript"');
    console.log('ğŸ¤– Model: llmgateway__OpenAIGPT5');
    console.log('ğŸ“Š Max tokens: 2048');
    console.log('\nğŸ“¥ Response:\n');

    let responseText = '';
    let chunkCount = 0;

    await client.chat([
      { role: 'user', content: 'Write hello world in Javascript' }
    ], {
      model: 'llmgateway__OpenAIGPT5',
      maxTokens: 2048,
      onContent: (content: string) => {
        process.stdout.write(content);
        responseText += content;
      },
      onChunk: (chunk: any) => {
        chunkCount++;
        if (chunk.data.generation_details?.parameters?.usage) {
          const usage = chunk.data.generation_details.parameters.usage;
          console.log(`\nğŸ“Š Usage: ${usage.total_tokens} tokens (${usage.prompt_tokens} prompt + ${usage.completion_tokens} completion)`);
        }
      },
      onError: (error: Error) => {
        console.error('\nâŒ Error:', error.message);
      },
      onEnd: () => {
        console.log('\n\nâœ… Streaming completed successfully!');
        console.log(`ğŸ“Š Total chunks received: ${chunkCount}`);
        console.log(`ğŸ“ Response length: ${responseText.length} characters`);
        console.log(`ğŸ”¤ Response preview: ${responseText.substring(0, 100)}...`);
      }
    });

  } catch (error) {
    console.error('âŒ Test failed:', error);
  }
}

async function testEnhancedStreaming() {
  console.log('\nğŸš€ Testing Enhanced Streaming Client\n');

  const client = new EnhancedStreamingClient({
    apiKey: '651192c5-37ff-440a-b930-7444c69f4422',
    tenantId: 'core/falcontest1-core4sdb6/00DSG000002tHLd2AM',
    baseUrl: 'https://test.api.salesforce.com',
    featureId: 'EinsteinForDevelopers',
    maxRetries: 3,
    retryDelay: 1000
  });

  try {
    console.log('ğŸ“¤ Sending request with progress tracking...');
    console.log('ğŸ“ Prompt: "Explain TypeScript benefits"');
    console.log('ğŸ¤– Model: llmgateway__OpenAIGPT5');
    console.log('ğŸ“Š Max tokens: 1000');
    console.log('\nğŸ“¥ Response:\n');

    const response = await client.streamWithProgress([
      { role: 'user', content: 'Explain TypeScript benefits over JavaScript in 3 key points' }
    ], {
      model: 'llmgateway__OpenAIGPT5',
      maxTokens: 1000,
      onProgress: (progress) => {
        process.stdout.write(`\rğŸ“Š Progress: ${progress.received} chunks received`);
      }
    });

    console.log(`\n\nâœ… Enhanced streaming completed!`);
    console.log(`ğŸ“ Full response: ${response}`);

  } catch (error) {
    console.error('âŒ Enhanced test failed:', error);
  }
}

async function testModelManager() {
  console.log('\nğŸ¯ Testing Streaming Model Manager\n');

  const manager = new StreamingModelManager();

  try {
    console.log('ğŸ“¤ Sending request via model manager...');
    console.log('ğŸ“ Prompt: "Create a simple Express.js route"');
    console.log('ğŸŒ Environment: test');
    console.log('ğŸ¤– Model: llmgateway__OpenAIGPT5');
    console.log('\nğŸ“¥ Response:\n');

    const response = await manager.stream([
      { role: 'user', content: 'Create a simple Express.js route that returns "Hello World"' }
    ], {
      environment: 'test',
      model: 'llmgateway__OpenAIGPT5',
      maxTokens: 800,
      onContent: (content: string) => {
        process.stdout.write(content);
      }
    });

    console.log(`\n\nâœ… Model manager test completed!`);
    console.log(`ğŸ“ Response length: ${response.length} characters`);

  } catch (error) {
    console.error('âŒ Model manager test failed:', error);
  }
}

async function testDifferentModels() {
  console.log('\nğŸ¤– Testing Different Models\n');

  const client = createStreamingClient({
    apiKey: '651192c5-37ff-440a-b930-7444c69f4422',
    tenantId: 'core/falcontest1-core4sdb6/00DSG000002tHLd2AM',
    baseUrl: 'https://test.api.salesforce.com',
    featureId: 'EinsteinForDevelopers'
  });

  const models = [
    { id: 'llmgateway__OpenAIGPT5', name: 'GPT-5' },
    { id: 'llmgateway__OpenAIGPT4o', name: 'GPT-4o' },
    { id: 'xgen_stream', name: 'XGen Stream' }
  ];

  for (const model of models) {
    console.log(`\nğŸ§ª Testing ${model.name} (${model.id})`);
    console.log('ğŸ“ Prompt: "Say hello and identify yourself"');
    console.log('ğŸ“Š Max tokens: 100');
    console.log('\nğŸ“¥ Response:\n');

    try {
      let responseText = '';
      
      await client.chat([
        { role: 'user', content: 'Say hello and identify yourself' }
      ], {
        model: model.id,
        maxTokens: 100,
        onContent: (content: string) => {
          process.stdout.write(content);
          responseText += content;
        },
        onError: (error: Error) => {
          console.error(`\nâŒ ${model.name} failed:`, error.message);
        },
        onEnd: () => {
          console.log(`\nâœ… ${model.name} completed: ${responseText.substring(0, 50)}...`);
        }
      });

    } catch (error) {
      console.error(`âŒ ${model.name} test failed:`, error);
    }
  }
}

// Main test runner
async function runAllTests() {
  console.log('ğŸš€ OpenAIStreamingWebClient Test Suite\n');
  console.log('=' .repeat(50));

  try {
    // Test 1: Basic streaming
    await testBasicStreaming();
    
    // Test 2: Enhanced streaming
    await testEnhancedStreaming();
    
    // Test 3: Model manager
    await testModelManager();
    
    // Test 4: Different models
    await testDifferentModels();

    console.log('\n' + '=' .repeat(50));
    console.log('âœ… All tests completed!');
    console.log('\nğŸ“‹ Test Summary:');
    console.log('  âœ… Basic streaming client');
    console.log('  âœ… Enhanced streaming with retry logic');
    console.log('  âœ… Streaming model manager');
    console.log('  âœ… Multiple model support');
    console.log('\nğŸ‰ OpenAIStreamingWebClient is working correctly!');

  } catch (error) {
    console.error('\nâŒ Test suite failed:', error);
    process.exit(1);
  }
}

// Run tests if this file is executed directly
if (require.main === module) {
  runAllTests().catch(console.error);
}

export { testBasicStreaming, testEnhancedStreaming, testModelManager, testDifferentModels, runAllTests };
